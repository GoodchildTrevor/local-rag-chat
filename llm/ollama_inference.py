import asyncio
import re

from config.prompts import SYSTEM_PROMPT
from llm.ollama_configs import (
    llm,
    PROMPT_TEMPLATE
)


async def ask_llm(
    logger,
    query: str,
    context: list[str],
    history: list[tuple[str, str]],
    results,  # reserved for future RAGAS implementation
) -> str:
    """
    Generate a response from the LLM given a user query, context, and history.
    :param logger: Logger instance for logging events.
    :param query: The user's current question or message.
    :param context: Retrieved documents or external context for the LLM.
    :param history: List of previous user–bot interaction pairs.
    :param results: Placeholder for future RAGAS-compatible results.
    :return: Cleaned string response generated by the LLM.
    """
    # Convert context and history to plain text strings
    context_text = "\n".join(context) if context else "—"
    history_text = "\n".join(f"Юзер: {u} | Бот: {b}" for u, b in history) if history else "—"
    # Format the full prompt using system instructions, context, history, and the user query
    prompt = PROMPT_TEMPLATE.format(
        system=SYSTEM_PROMPT,
        context=context_text,
        history=history_text,
        query=query,
    )
    logger.info("ask_llm: prompt=%r", prompt.replace("\n", "\\n"))
    # Generate a raw response from the model in a background thread
    response_obj = await asyncio.to_thread(lambda: llm.complete(prompt))
    # Try extracting the actual text response
    text = getattr(response_obj, "text", None) or getattr(response_obj.message, "content", str(response_obj))
    logger.info("ask_llm: got response text=%r", text)
    # Remove any special tokens (e.g., <think>) and return cleaned response
    response = re.sub(r'(?s)^<think>.*?</think>\s*', '', text)
    logger.info("final_response=%r", response)

    return response
